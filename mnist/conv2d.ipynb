{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../data/mnist/ext_train_manipulated.csv', newline='') as csvfile:\n",
    "    csv_file_object = csv.reader(csvfile, dialect='excel')\n",
    "    header = csv_file_object.__next__()\n",
    "    \n",
    "    data=[]\n",
    "    for row in csv_file_object:\n",
    "        data.append(row)\n",
    "    data = np.array(data)\n",
    "    data = data.astype(np.float)\n",
    "    \n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define constants\n",
    "NUM_CLASSES = 10\n",
    "INPUT_SIZE = 784\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "EVAL_BATCH_SIZE = 1\n",
    "\n",
    "HIDDEN1_UNITS = 512\n",
    "# HIDDEN2_UNITS = 128\n",
    "\n",
    "MAX_EPOCHS = 10\n",
    "\n",
    "CONV1_STRIDES = [1, 1, 1, 1]\n",
    "CONV2_STRIDES = [1, 1, 1, 1]\n",
    "\n",
    "TRAIN_DIR = \"./checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inference Graph\n",
    "def mnist_inference(image, hidden1_units, hidden2_units):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        image: Image placeholder\n",
    "        hidden1_units: Size of the first hidden layer\n",
    "        hidden2_units: Size of the second hidden layer\n",
    "    Returns:\n",
    "        logits: Output tensor with the computed logits\n",
    "    \"\"\"\n",
    "    # First Conv Layer\n",
    "    with tf.name_scope('conv1'):\n",
    "        filters = tf.Variable(\n",
    "                tf.truncated_normal([5, 5, 1, 32],\n",
    "                                    stddev=1.0 / math.sqrt(float(5 * 5 * 32))),\n",
    "                name='filters')\n",
    "        biases = tf.Variable(tf.zeros([32]),\n",
    "                             name='biases')\n",
    "        conv = tf.nn.conv2d(image, filters, strides=CONV1_STRIDES, padding='SAME')\n",
    "        conv1 = tf.nn.relu(conv + biases)\n",
    "        pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    # Second Conv Layer\n",
    "    with tf.name_scope('conv2'):\n",
    "        filters = tf.Variable(\n",
    "                tf.truncated_normal([5, 5, 32, 64],\n",
    "                                    stddev=1.0 / math.sqrt(float(5 * 5 * 64))),\n",
    "                name='filters')\n",
    "        biases = tf.Variable(tf.zeros([64]),\n",
    "                            name='biases')\n",
    "        conv = tf.nn.conv2d(pool1, filters, strides=CONV2_STRIDES, padding='SAME')\n",
    "        conv2 = tf.nn.relu(conv + biases)\n",
    "        pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    # Hidden 1\n",
    "    with tf.name_scope('hidden1'):\n",
    "        weights = tf.Variable(\n",
    "                tf.truncated_normal([7 * 7 * 64, hidden1_units],\n",
    "                                    stddev=1.0 / math.sqrt(float(7 * 7 * 64))),\n",
    "                name='weights')\n",
    "        biases = tf.Variable(tf.zeros([hidden1_units]),\n",
    "                             name='biases')\n",
    "        pool2_flat = tf.reshape(pool2, (-1, 7 * 7 * 64))\n",
    "        hidden1 = tf.nn.relu(tf.matmul(pool2_flat, weights) + biases)\n",
    "    # Linear\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        weights = tf.Variable(\n",
    "                tf.truncated_normal([hidden1_units, NUM_CLASSES],\n",
    "                                    stddev=1.0 / math.sqrt(float(hidden1_units))),\n",
    "                name='weights')\n",
    "        biases = tf.Variable(tf.zeros([NUM_CLASSES]),\n",
    "                             name='biases')\n",
    "        logits = tf.matmul(hidden1, weights) + biases\n",
    "        \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build training graph\n",
    "def mnist_training(logits, labels, learning_rate):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: Logits tensor, float - [BATCH_SIZE, NUM_CLASSES]\n",
    "        labels: Labels tensor, int32 - [BATCH_SIZE]\n",
    "        learning_rate: The learning rate to use for gradient descent\n",
    "    Returns:\n",
    "        train_op: The Op for training\n",
    "        loss: The Op for calculating loss\n",
    "    \"\"\"\n",
    "    labels = tf.to_int64(labels)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits, labels, name='xentropy')\n",
    "    loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    \n",
    "    return train_op, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build complete graph\n",
    "mnist_graph = tf.Graph()\n",
    "with mnist_graph.as_default():\n",
    "    # Generate placeholders for the images and labels\n",
    "    inputs_placeholder = tf.placeholder(tf.float32)\n",
    "    labels_placeholder = tf.placeholder(tf.int32)\n",
    "    tf.add_to_collection(\"inputs\", inputs_placeholder)\n",
    "    tf.add_to_collection(\"labels\", labels_placeholder)\n",
    "    \n",
    "    # Build a Graph that computes predictions from the inference model\n",
    "    logits = mnist_inference(inputs_placeholder,\n",
    "                             HIDDEN1_UNITS,\n",
    "                             1)\n",
    "    tf.add_to_collection(\"logits\", logits)\n",
    "    \n",
    "    # Add to graph the Ops that calculate and apply gradients\n",
    "    train_op, loss = mnist_training(logits, labels_placeholder, 0.03)\n",
    "    \n",
    "    # Add the variable initializer Op\n",
    "    init = tf.initialize_all_variables()\n",
    "    \n",
    "    # Create a saver for writing training checkpoints\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run training for MAX_STEPS and save checkpoint at the end\n",
    "with tf.Session(graph=mnist_graph) as sess:\n",
    "    # Run the Op to initialize variables\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Start the training loop\n",
    "    losses = []\n",
    "    for step in range(MAX_EPOCHS):\n",
    "        # Generate batchs of image\n",
    "        np.random.shuffle(data)\n",
    "        remainder = len(data) % BATCH_SIZE\n",
    "        if remainder != 0:\n",
    "            epoch = data[:-(len(data) % BATCH_SIZE)]\n",
    "        else:\n",
    "            epoch = data\n",
    "        batchs = np.split(epoch, len(epoch) / BATCH_SIZE)\n",
    "        batchs_num = len(batchs)\n",
    "        \n",
    "        batch_loss = 0.\n",
    "        i = 0\n",
    "        print('Itertate epoch : ', step)\n",
    "        for batch in batchs:\n",
    "            images_feed = batch[0::, 1:]\n",
    "            labels_feed = batch[0::, 0].astype(np.int)\n",
    "            images_feed = np.array(images_feed).reshape((-1, 28, 28, 1))\n",
    "            labels_feed = np.array(labels_feed)\n",
    "        \n",
    "            _, loss_value = sess.run([train_op, loss],\n",
    "                                     feed_dict={inputs_placeholder: images_feed,\n",
    "                                                labels_placeholder: labels_feed})\n",
    "            \n",
    "            batch_loss += loss_value / batchs_num\n",
    "            losses.append(loss_value)\n",
    "            if i % 20 == 0:\n",
    "                print('  current batch : ', i)\n",
    "            i = i + 1\n",
    "            \n",
    "        print('Epoch %d: loss = %.2f' % (step, batch_loss))\n",
    "        \n",
    "    # Write a checkpoint\n",
    "    checkpoint_file = os.path.join(TRAIN_DIR, 'checkpoint')\n",
    "    saver.save(sess, checkpoint_file, global_step=step)\n",
    "    plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../data/mnist/test_manipulated.csv', newline='') as csvfile:\n",
    "    csv_file_object = csv.reader(csvfile, dialect='excel')\n",
    "    haeder = csv_file_object.__next__()\n",
    "    \n",
    "    test=[]\n",
    "    for row in csv_file_object:\n",
    "        test.append(row)\n",
    "    test = np.array(test)\n",
    "    test = test.astype(np.float)\n",
    "\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    saver = tf.train.import_meta_graph(\n",
    "        os.path.join(TRAIN_DIR, 'checkpoint-9.meta'))\n",
    "    saver.restore(\n",
    "        sess, os.path.join(TRAIN_DIR, 'checkpoint-9'))\n",
    "    \n",
    "    # Retrieve Ops\n",
    "    logits = tf.get_collection('logits')[0]\n",
    "    inputs_placeholder = tf.get_collection('inputs')[0]\n",
    "    labels_placeholder = tf.get_collection('labels')[0]\n",
    "    \n",
    "    eval_op = tf.nn.top_k(logits)\n",
    "    \n",
    "    # Run evaluation\n",
    "    labels_feed = test[0::, 0]\n",
    "    inputs_feed = test.reshape((-1, 28, 28, 1))\n",
    "    labels_feed = np.array(labels_feed)\n",
    "    imgplot = plt.imshow(np.reshape(inputs_feed[0], (28, 28)))\n",
    "    \n",
    "    prediction = sess.run(eval_op,\n",
    "                          feed_dict={inputs_placeholder: inputs_feed,\n",
    "                                    labels_placeholder: labels_feed})\n",
    "    print('prediction : ', prediction.indices[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('conv2d.csv', 'w', newline='') as csvfile:\n",
    "    csv_file_object = csv.writer(csvfile, dialect='excel')\n",
    "    csv_file_object.writerow(['ImageId', 'Label'])\n",
    "    for i in range(len(test)):\n",
    "        csv_file_object.writerow([i+1, prediction.indices[i][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
