{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import csv\n",
    "import random\n",
    "from six.moves import xrange\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']\n"
     ]
    }
   ],
   "source": [
    "with open('./csv/train.csv', newline='') as csvfile:\n",
    "    csv_file_object = csv.reader(csvfile, dialect='excel')\n",
    "    header = csv_file_object.__next__()\n",
    "    \n",
    "    data=[]                          # Create a variable called 'data'.\n",
    "    for row in csv_file_object:      # Run through each row in the csv file,\n",
    "        data.append(row)             # adding each row to the data variable\n",
    "    data = np.array(data)            # Then convert from a list to an array\n",
    "\n",
    "h1 = header[1:3]\n",
    "h2 = header[4:8]\n",
    "h3 = header[9]\n",
    "h1.extend(h2)\n",
    "h1.append(h3)\n",
    "header = h1\n",
    "data = data[0::, [1, 2, 4, 5, 6, 7, 9]]\n",
    "data[ data[0::, 2] == 'female', 2] = 0\n",
    "data[ data[0::, 2] == 'male', 2] = 1\n",
    "data[ data[0::, 3] == '', 3] = 0\n",
    "\n",
    "data = data.astype(np.float)\n",
    "\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n"
     ]
    }
   ],
   "source": [
    "with open('./csv/test.csv', newline='') as csvfile:\n",
    "    csv_file_object = csv.reader(csvfile, dialect='excel')\n",
    "    header = csv_file_object.__next__()\n",
    "    \n",
    "    test=[]\n",
    "    for row in csv_file_object:\n",
    "        test.append(row)\n",
    "    test = np.array(test)\n",
    "\n",
    "ids = test[0::, 0]\n",
    "test = test[0::, [1, 3, 4, 5, 6, 8]]\n",
    "test[ test[0::, 1] == 'female', 1] = 0\n",
    "test[ test[0::, 1] == 'male', 1] = 1\n",
    "test[ test[0::, 2] == '', 2] = 0\n",
    "test[ test[0::, 5] == '', 5] = 0\n",
    "\n",
    "test = test.astype(float)\n",
    "\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2.2 Define some constants.\n",
    "# The MNIST dataset has 10 classes, representing the digits 0 through 9.\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# The MNIST images are always 28x28 pixels.\n",
    "INPUT_SIZE = 6\n",
    "\n",
    "# Batch size. Must be evenly dividable by dataset sizes.\n",
    "BATCH_SIZE = 60\n",
    "EVAL_BATCH_SIZE = 1\n",
    "\n",
    "# Number of units in hidden layers.\n",
    "HIDDEN1_UNITS = 128\n",
    "HIDDEN2_UNITS = 128\n",
    "\n",
    "# Maximum number of training steps.\n",
    "MAX_STEPS = 300000\n",
    "\n",
    "# Directory to put the trained data.\n",
    "TRAIN_DIR=\"/tmp/titanic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2.4 Build inference graph.\n",
    "def mnist_inference(inputs, hidden1_units, hidden2_units):\n",
    "    \"\"\"Build the MNIST model up to where it may be used for inference.\n",
    "    Args:\n",
    "        images: Images placeholder.\n",
    "        hidden1_units: Size of the first hidden layer.\n",
    "        hidden2_units: Size of the second hidden layer.\n",
    "    Returns:\n",
    "        logits: Output tensor with the computed logits.\n",
    "    \"\"\"\n",
    "    # Hidden 1\n",
    "    with tf.name_scope('hidden1'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([INPUT_SIZE, hidden1_units],\n",
    "                                stddev=1.0 / math.sqrt(float(INPUT_SIZE))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([hidden1_units]),\n",
    "                             name='biases')\n",
    "        hidden1 = tf.nn.relu(tf.matmul(inputs, weights) + biases)\n",
    "    # Hidden 2\n",
    "    with tf.name_scope('hidden2'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([hidden1_units, hidden2_units],\n",
    "                                stddev=1.0 / math.sqrt(float(hidden1_units))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([hidden2_units]),\n",
    "                             name='biases')\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "    # Linear\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([hidden2_units, NUM_CLASSES],\n",
    "                                stddev=1.0 / math.sqrt(float(hidden2_units))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([NUM_CLASSES]),\n",
    "                             name='biases')\n",
    "        logits = tf.matmul(hidden2, weights) + biases\n",
    "\n",
    "    # Uncomment the following line to see what we have constructed.\n",
    "    # tf.train.write_graph(tf.get_default_graph().as_graph_def(),\n",
    "    #                      \"/tmp\", \"inference.pbtxt\", as_text=True)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2.5 Build training graph.\n",
    "def mnist_training(logits, labels, learning_rate):\n",
    "    \"\"\"Build the training graph.\n",
    "\n",
    "    Args:\n",
    "        logits: Logits tensor, float - [BATCH_SIZE, NUM_CLASSES].\n",
    "        labels: Labels tensor, int32 - [BATCH_SIZE], with values in the\n",
    "          range [0, NUM_CLASSES).\n",
    "        learning_rate: The learning rate to use for gradient descent.\n",
    "    Returns:\n",
    "        train_op: The Op for training.\n",
    "        loss: The Op for calculating loss.\n",
    "    \"\"\"\n",
    "    # Create an operation that calculates loss.\n",
    "    labels = tf.to_int64(labels)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits, labels, name='xentropy')\n",
    "    loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "    # Create the gradient descent optimizer with the given learning rate.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    # Create a variable to track the global step.\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    # Use the optimizer to apply the gradients that minimize the loss\n",
    "    # (and also increment the global step counter) as a single training step.\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Uncomment the following line to see what we have constructed.\n",
    "#     tf.train.write_graph(tf.get_default_graph().as_graph_def(),\n",
    "#                          \"/tmp\", \"train.pbtxt\", as_text=True)\n",
    "\n",
    "    return train_op, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2.6 Build the complete graph for feeding inputs, training, and saving checkpoints.\n",
    "mnist_graph = tf.Graph()\n",
    "with mnist_graph.as_default():\n",
    "    # Generate placeholders for the images and labels.\n",
    "    inputs_placeholder = tf.placeholder(tf.float32)                                       \n",
    "    labels_placeholder = tf.placeholder(tf.int32)\n",
    "    tf.add_to_collection(\"inputs\", inputs_placeholder)  # Remember this Op.\n",
    "    tf.add_to_collection(\"labels\", labels_placeholder)  # Remember this Op.\n",
    "\n",
    "    # Build a Graph that computes predictions from the inference model.\n",
    "    logits = mnist_inference(inputs_placeholder,\n",
    "                             HIDDEN1_UNITS,\n",
    "                             HIDDEN2_UNITS)\n",
    "    tf.add_to_collection(\"logits\", logits)  # Remember this Op.\n",
    "\n",
    "    # Add to the Graph the Ops that calculate and apply gradients.\n",
    "    train_op, loss = mnist_training(logits, labels_placeholder, 0.01)\n",
    "\n",
    "    # Add the variable initializer Op.\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    # Create a saver for writing training checkpoints.\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # Uncomment the following line to see what we have constructed.\n",
    "    # tf.train.write_graph(tf.get_default_graph().as_graph_def(),\n",
    "    #                      \"/tmp\", \"complete.pbtxt\", as_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss = 1.03\n",
      "[ 1.  0.  1.  0.  1.  0.  0.  1.  0.  0.  0.  1.  0.  0.  1.  1.  1.  0.\n",
      "  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  1.  1.\n",
      "  0.  1.  1.  1.  0.  1.  1.  0.  0.  1.  0.  0.  1.  0.  0.  1.  0.  0.\n",
      "  1.  0.  0.  0.  0.  0.]\n",
      "Step 1000: loss = 0.50\n",
      "Step 2000: loss = 0.71\n",
      "Step 3000: loss = 0.51\n",
      "Step 4000: loss = 0.56\n",
      "Step 5000: loss = 0.41\n",
      "Step 6000: loss = 0.59\n",
      "Step 7000: loss = 0.38\n",
      "Step 8000: loss = 0.56\n",
      "Step 9000: loss = 0.44\n",
      "Step 10000: loss = 0.43\n",
      "Step 11000: loss = 0.33\n",
      "Step 12000: loss = 0.45\n",
      "Step 13000: loss = 0.31\n",
      "Step 14000: loss = 0.44\n",
      "Step 15000: loss = 0.33\n",
      "Step 16000: loss = 0.32\n",
      "Step 17000: loss = 0.46\n",
      "Step 18000: loss = 0.47\n",
      "Step 19000: loss = 0.31\n",
      "Step 20000: loss = 0.41\n",
      "Step 21000: loss = 0.54\n",
      "Step 22000: loss = 0.31\n",
      "Step 23000: loss = 0.30\n",
      "Step 24000: loss = 0.55\n",
      "Step 25000: loss = 0.36\n",
      "Step 26000: loss = 0.35\n",
      "Step 27000: loss = 0.34\n",
      "Step 28000: loss = 0.29\n",
      "Step 29000: loss = 0.26\n",
      "Step 30000: loss = 0.39\n",
      "Step 31000: loss = 0.20\n",
      "Step 32000: loss = 0.30\n",
      "Step 33000: loss = 0.31\n",
      "Step 34000: loss = 0.32\n",
      "Step 35000: loss = 0.43\n",
      "Step 36000: loss = 0.23\n",
      "Step 37000: loss = 0.39\n",
      "Step 38000: loss = 0.29\n",
      "Step 39000: loss = 0.32\n",
      "Step 40000: loss = 0.23\n",
      "Step 41000: loss = 0.33\n",
      "Step 42000: loss = 0.26\n",
      "Step 43000: loss = 0.33\n",
      "Step 44000: loss = 0.27\n",
      "Step 45000: loss = 0.35\n",
      "Step 46000: loss = 0.24\n",
      "Step 47000: loss = 0.36\n",
      "Step 48000: loss = 0.18\n",
      "Step 49000: loss = 0.36\n",
      "Step 50000: loss = 0.34\n",
      "Step 51000: loss = 0.28\n",
      "Step 52000: loss = 0.24\n",
      "Step 53000: loss = 0.31\n",
      "Step 54000: loss = 0.22\n",
      "Step 55000: loss = 0.17\n",
      "Step 56000: loss = 0.43\n",
      "Step 57000: loss = 0.21\n",
      "Step 58000: loss = 0.24\n",
      "Step 59000: loss = 0.23\n",
      "Step 60000: loss = 0.23\n",
      "Step 61000: loss = 0.29\n",
      "Step 62000: loss = 0.41\n",
      "Step 63000: loss = 0.33\n",
      "Step 64000: loss = 0.29\n",
      "Step 65000: loss = 0.41\n",
      "Step 66000: loss = 0.28\n",
      "Step 67000: loss = 0.38\n",
      "Step 68000: loss = 0.28\n",
      "Step 69000: loss = 0.28\n",
      "Step 70000: loss = 0.54\n",
      "Step 71000: loss = 0.17\n",
      "Step 72000: loss = 0.24\n",
      "Step 73000: loss = 0.32\n",
      "Step 74000: loss = 0.24\n",
      "Step 75000: loss = 0.37\n",
      "Step 76000: loss = 0.23\n",
      "Step 77000: loss = 0.31\n",
      "Step 78000: loss = 0.20\n",
      "Step 79000: loss = 0.66\n",
      "Step 80000: loss = 0.29\n",
      "Step 81000: loss = 0.29\n",
      "Step 82000: loss = 0.23\n",
      "Step 83000: loss = 0.30\n",
      "Step 84000: loss = 0.09\n",
      "Step 85000: loss = 0.15\n",
      "Step 86000: loss = 0.25\n",
      "Step 87000: loss = 0.18\n",
      "Step 88000: loss = 0.22\n",
      "Step 89000: loss = 0.32\n",
      "Step 90000: loss = 0.15\n",
      "Step 91000: loss = 0.23\n",
      "Step 92000: loss = 0.41\n",
      "Step 93000: loss = 0.25\n",
      "Step 94000: loss = 0.24\n",
      "Step 95000: loss = 0.19\n",
      "Step 96000: loss = 0.19\n",
      "Step 97000: loss = 0.15\n",
      "Step 98000: loss = 0.26\n",
      "Step 99000: loss = 0.28\n",
      "Step 100000: loss = 0.23\n",
      "Step 101000: loss = 0.22\n",
      "Step 102000: loss = 0.29\n",
      "Step 103000: loss = 0.26\n",
      "Step 104000: loss = 0.13\n",
      "Step 105000: loss = 0.25\n",
      "Step 106000: loss = 0.14\n",
      "Step 107000: loss = 0.23\n",
      "Step 108000: loss = 0.24\n",
      "Step 109000: loss = 0.19\n",
      "Step 110000: loss = 0.19\n",
      "Step 111000: loss = 0.15\n",
      "Step 112000: loss = 0.28\n",
      "Step 113000: loss = 0.23\n",
      "Step 114000: loss = 0.20\n",
      "Step 115000: loss = 0.22\n",
      "Step 116000: loss = 0.29\n",
      "Step 117000: loss = 0.10\n",
      "Step 118000: loss = 0.18\n",
      "Step 119000: loss = 0.28\n",
      "Step 120000: loss = 0.19\n",
      "Step 121000: loss = 0.22\n",
      "Step 122000: loss = 0.26\n",
      "Step 123000: loss = 0.21\n",
      "Step 124000: loss = 0.17\n",
      "Step 125000: loss = 0.22\n",
      "Step 126000: loss = 0.19\n",
      "Step 127000: loss = 0.30\n",
      "Step 128000: loss = 0.14\n",
      "Step 129000: loss = 0.21\n",
      "Step 130000: loss = 0.17\n",
      "Step 131000: loss = 0.26\n",
      "Step 132000: loss = 0.32\n",
      "Step 133000: loss = 0.45\n",
      "Step 134000: loss = 0.20\n",
      "Step 135000: loss = 0.19\n",
      "Step 136000: loss = 0.29\n",
      "Step 137000: loss = 0.15\n",
      "Step 138000: loss = 0.19\n",
      "Step 139000: loss = 0.25\n",
      "Step 140000: loss = 0.20\n",
      "Step 141000: loss = 0.23\n",
      "Step 142000: loss = 0.24\n",
      "Step 143000: loss = 0.22\n",
      "Step 144000: loss = 0.17\n",
      "Step 145000: loss = 0.23\n",
      "Step 146000: loss = 0.19\n",
      "Step 147000: loss = 0.13\n",
      "Step 148000: loss = 0.22\n",
      "Step 149000: loss = 0.20\n",
      "Step 150000: loss = 0.30\n",
      "Step 151000: loss = 0.25\n",
      "Step 152000: loss = 0.14\n",
      "Step 153000: loss = 0.16\n",
      "Step 154000: loss = 0.25\n",
      "Step 155000: loss = 0.29\n",
      "Step 156000: loss = 0.19\n",
      "Step 157000: loss = 0.12\n",
      "Step 158000: loss = 0.33\n",
      "Step 159000: loss = 0.28\n",
      "Step 160000: loss = 0.20\n",
      "Step 161000: loss = 0.25\n",
      "Step 162000: loss = 0.19\n",
      "Step 163000: loss = 0.16\n",
      "Step 164000: loss = 0.21\n",
      "Step 165000: loss = 0.21\n",
      "Step 166000: loss = 0.12\n",
      "Step 167000: loss = 0.14\n",
      "Step 168000: loss = 0.27\n",
      "Step 169000: loss = 0.15\n",
      "Step 170000: loss = 0.17\n",
      "Step 171000: loss = 0.24\n",
      "Step 172000: loss = 0.12\n",
      "Step 173000: loss = 0.14\n",
      "Step 174000: loss = 0.17\n",
      "Step 175000: loss = 0.22\n",
      "Step 176000: loss = 0.22\n",
      "Step 177000: loss = 0.26\n",
      "Step 178000: loss = 0.30\n",
      "Step 179000: loss = 0.19\n",
      "Step 180000: loss = 0.28\n",
      "Step 181000: loss = 0.23\n",
      "Step 182000: loss = 0.27\n",
      "Step 183000: loss = 0.14\n",
      "Step 184000: loss = 0.17\n",
      "Step 185000: loss = 0.19\n",
      "Step 186000: loss = 0.21\n",
      "Step 187000: loss = 0.23\n",
      "Step 188000: loss = 0.14\n",
      "Step 189000: loss = 0.26\n",
      "Step 190000: loss = 0.18\n",
      "Step 191000: loss = 0.15\n",
      "Step 192000: loss = 1.05\n",
      "Step 193000: loss = 0.24\n",
      "Step 194000: loss = 0.22\n",
      "Step 195000: loss = 0.17\n",
      "Step 196000: loss = 0.09\n",
      "Step 197000: loss = 0.19\n",
      "Step 198000: loss = 0.18\n",
      "Step 199000: loss = 0.23\n",
      "Step 200000: loss = 0.17\n",
      "Step 201000: loss = 0.19\n",
      "Step 202000: loss = 0.24\n",
      "Step 203000: loss = 0.26\n",
      "Step 204000: loss = 0.20\n",
      "Step 205000: loss = 0.14\n",
      "Step 206000: loss = 0.12\n",
      "Step 207000: loss = 0.13\n",
      "Step 208000: loss = 0.20\n",
      "Step 209000: loss = 0.28\n",
      "Step 210000: loss = 0.17\n",
      "Step 211000: loss = 0.25\n",
      "Step 212000: loss = 0.17\n",
      "Step 213000: loss = 0.15\n",
      "Step 214000: loss = 0.17\n",
      "Step 215000: loss = 0.21\n",
      "Step 216000: loss = 0.21\n",
      "Step 217000: loss = 0.11\n",
      "Step 218000: loss = 0.21\n",
      "Step 219000: loss = 0.21\n",
      "Step 220000: loss = 0.17\n",
      "Step 221000: loss = 0.16\n",
      "Step 222000: loss = 0.20\n",
      "Step 223000: loss = 0.13\n",
      "Step 224000: loss = 0.23\n",
      "Step 225000: loss = 0.40\n",
      "Step 226000: loss = 0.20\n",
      "Step 227000: loss = 0.17\n",
      "Step 228000: loss = 0.19\n",
      "Step 229000: loss = 0.13\n",
      "Step 230000: loss = 0.24\n",
      "Step 231000: loss = 0.11\n",
      "Step 232000: loss = 0.25\n",
      "Step 233000: loss = 0.19\n",
      "Step 234000: loss = 0.17\n",
      "Step 235000: loss = 0.28\n",
      "Step 236000: loss = 0.24\n",
      "Step 237000: loss = 0.13\n",
      "Step 238000: loss = 0.17\n",
      "Step 239000: loss = 0.28\n",
      "Step 240000: loss = 0.12\n",
      "Step 241000: loss = 0.17\n",
      "Step 242000: loss = 0.10\n",
      "Step 243000: loss = 0.14\n",
      "Step 244000: loss = 0.25\n",
      "Step 245000: loss = 0.12\n",
      "Step 246000: loss = 0.22\n",
      "Step 247000: loss = 0.12\n",
      "Step 248000: loss = 0.14\n",
      "Step 249000: loss = 0.15\n",
      "Step 250000: loss = 0.19\n",
      "Step 251000: loss = 0.14\n",
      "Step 252000: loss = 0.27\n",
      "Step 253000: loss = 0.14\n",
      "Step 254000: loss = 0.15\n",
      "Step 255000: loss = 0.14\n",
      "Step 256000: loss = 0.15\n",
      "Step 257000: loss = 0.28\n",
      "Step 258000: loss = 0.24\n",
      "Step 259000: loss = 0.18\n",
      "Step 260000: loss = 0.12\n",
      "Step 261000: loss = 0.19\n",
      "Step 262000: loss = 0.10\n",
      "Step 263000: loss = 0.17\n",
      "Step 264000: loss = 0.22\n",
      "Step 265000: loss = 0.18\n",
      "Step 266000: loss = 0.10\n",
      "Step 267000: loss = 0.13\n",
      "Step 268000: loss = 0.17\n",
      "Step 269000: loss = 0.24\n",
      "Step 270000: loss = 0.20\n",
      "Step 271000: loss = 0.15\n",
      "Step 272000: loss = 0.11\n",
      "Step 273000: loss = 0.13\n",
      "Step 274000: loss = 0.18\n",
      "Step 275000: loss = 0.16\n",
      "Step 276000: loss = 0.15\n",
      "Step 277000: loss = 0.27\n",
      "Step 278000: loss = 0.15\n",
      "Step 279000: loss = 0.15\n",
      "Step 280000: loss = 0.17\n",
      "Step 281000: loss = 0.33\n",
      "Step 282000: loss = 0.19\n",
      "Step 283000: loss = 0.16\n",
      "Step 284000: loss = 0.18\n",
      "Step 285000: loss = 0.21\n",
      "Step 286000: loss = 0.23\n",
      "Step 287000: loss = 0.17\n",
      "Step 288000: loss = 0.18\n",
      "Step 289000: loss = 0.36\n",
      "Step 290000: loss = 0.21\n",
      "Step 291000: loss = 0.21\n",
      "Step 292000: loss = 0.16\n",
      "Step 293000: loss = 0.22\n",
      "Step 294000: loss = 0.31\n",
      "Step 295000: loss = 0.08\n",
      "Step 296000: loss = 0.15\n",
      "Step 297000: loss = 0.22\n",
      "Step 298000: loss = 0.11\n",
      "Step 299000: loss = 0.13\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x102843c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2.7 Run training for MAX_STEPS and save checkpoint at the end.\n",
    "with tf.Session(graph=mnist_graph) as sess:\n",
    "    # Run the Op to initialize the variables.\n",
    "    sess.run(init)\n",
    "\n",
    "    # Start the training loop.\n",
    "    losses = []\n",
    "    for step in xrange(MAX_STEPS):\n",
    "        # Read a batch of images and labels.\n",
    "#         if step == 0:\n",
    "#             print(data)\n",
    "        np.random.shuffle(data)\n",
    "        batch = data[:BATCH_SIZE]\n",
    "        inputs_feed = batch[0::, 1:]\n",
    "        labels_feed = batch[0::, 0]\n",
    "        \n",
    "        if step == 100:\n",
    "            print(labels_feed)\n",
    "\n",
    "        # Run one step of the model.  The return values are the activations\n",
    "        # from the `train_op` (which is discarded) and the `loss` Op.  To\n",
    "        # inspect the values of your Ops or variables, you may include them\n",
    "        # in the list passed to sess.run() and the value tensors will be\n",
    "        # returned in the tuple from the call.\n",
    "        _, loss_value = sess.run([train_op, loss],\n",
    "                                 feed_dict={inputs_placeholder: inputs_feed, labels_placeholder: labels_feed})\n",
    "\n",
    "        losses.append(loss_value)\n",
    "        # Print out loss value.\n",
    "        if step % 1000 == 0:\n",
    "            print('Step %d: loss = %.2f' % (step, loss_value))\n",
    "\n",
    "    # Write a checkpoint.\n",
    "    checkpoint_file = os.path.join(TRAIN_DIR, 'checkpoint')\n",
    "    saver.save(sess, checkpoint_file, global_step=step)\n",
    "    plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2.8 Run evaluation based on the saved checkpoint.\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    saver = tf.train.import_meta_graph(\n",
    "        os.path.join(TRAIN_DIR, \"checkpoint-1999.meta\"))\n",
    "    saver.restore(\n",
    "        sess, os.path.join(TRAIN_DIR, \"checkpoint-1999\"))\n",
    "\n",
    "    # Retrieve the Ops we 'remembered'.\n",
    "    logits = tf.get_collection(\"logits\")[0]\n",
    "    inputs_placeholder = tf.get_collection(\"inputs\")[0]\n",
    "    labels_placeholder = tf.get_collection(\"labels\")[0]\n",
    "    \n",
    "    # Add an Op that chooses the top k predictions.\n",
    "    eval_op = tf.nn.top_k(logits)\n",
    "    \n",
    "    # Run evaluation.\n",
    "    inputs_feed = test\n",
    "    labels_feed = test[0::, 0].astype(np.int)\n",
    "\n",
    "    prediction = sess.run(eval_op,\n",
    "                          feed_dict={inputs_placeholder: inputs_feed,\n",
    "                                     labels_placeholder: labels_feed})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('simple_nn.csv', 'w', newline='') as csvfile:\n",
    "    csv_file_object = csv.writer(csvfile, dialect='excel')\n",
    "    csv_file_object.writerow([\"PassengerId\", \"Survived\"])\n",
    "    for i in xrange(ids.size):\n",
    "        csv_file_object.writerow([ids[i], 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
